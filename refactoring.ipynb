{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Mujoco-based envs failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'mjrl'\n",
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "/home/magenta1223/anaconda3/envs/skill2/lib/python3.8/site-packages/glfw/__init__.py:916: GLFWError: (65544) b'X11: The DISPLAY environment variable is missing'\n",
      "  warnings.warn(message, GLFWError)\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'dotmap'\n",
      "pybullet build time: May 20 2022 19:44:17\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# from proposed.utils import get_dist\n",
    "from LVD.utils import get_dist\n",
    "\n",
    "\n",
    "import math\n",
    "from LVD.modules.base import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "class BaseModule(nn.Module):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(BaseModule, self).__init__()\n",
    "        self.set_attrs(config)\n",
    "        self._device = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    # set configs\n",
    "    def set_attrs(self, config = None):\n",
    "        if config is not None:\n",
    "            try:\n",
    "                for k, v in config.attrs.items():\n",
    "                    setattr(self, k, deepcopy(v))\n",
    "            except:\n",
    "                for k, v in config.items():\n",
    "                    setattr(self, k, deepcopy(v))           \n",
    "\n",
    "    def forward(self, x):\n",
    "        return NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._device.device\n",
    "\n",
    "\n",
    "\n",
    "class SequentialBuilder(BaseModule):\n",
    "    def __init__(self, config : Dict[str, None]):\n",
    "        super().__init__(config)\n",
    "        self.build()\n",
    "        self.explore = None\n",
    "\n",
    "    def get(self, name):\n",
    "        if not name : #name is None or not name:\n",
    "            return None\n",
    "        return getattr(self, name)\n",
    "\n",
    "    def layerbuild(self, attr_list, repeat = None):\n",
    "        build =  [[ self.get(attr_nm) for attr_nm in attr_list  ]]\n",
    "        if repeat is not None:\n",
    "            build = build * repeat\n",
    "        return build \n",
    "\n",
    "    def get_build(self):\n",
    "        if self.module_type == \"rnn\":\n",
    "            build = self.layerbuild([\"linear_cls\", \"in_feature\", \"hidden_dim\", None, \"act_cls\", \"bias\"])\n",
    "            build += self.layerbuild([\"rnn_cls\", \"hidden_dim\", \"hidden_dim\", \"n_blocks\", \"bias\", \"batch_first\", \"dropout\"])\n",
    "            build += self.layerbuild([\"linear_cls\", \"hidden_dim\", \"out_dim\", None, None, \"false\"])\n",
    "\n",
    "        elif self.module_type == \"linear\":\n",
    "            build = self.layerbuild([\"linear_cls\", \"in_feature\", \"hidden_dim\", None, \"act_cls\", \"bias\", \"dropout\"])\n",
    "            build += self.layerbuild([\"linear_cls\", \"hidden_dim\", \"hidden_dim\", \"norm_cls\", \"act_cls\"], self.get(\"n_blocks\"))\n",
    "            build += self.layerbuild([\"linear_cls\", \"hidden_dim\", \"out_dim\", None, None,  \"bias\", \"dropout\"])\n",
    "        else:\n",
    "            build = NotImplementedError\n",
    "\n",
    "        return build\n",
    "\n",
    "    def build(self):\n",
    "        build = self.get_build()\n",
    "        layers = []\n",
    "        for args in build:\n",
    "            cls, args = args[0], args[1:]\n",
    "            layers.append(cls(*args))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "\n",
    "    # -------------------- mdoelign -------------------- # \n",
    "    \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        out = x \n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "            if isinstance(out, tuple): # rnn\n",
    "                out = out[0]\n",
    "        return out\n",
    "\n",
    "\n",
    "    def dist(self, *args, detached = False):\n",
    "        result = self(*args)\n",
    "\n",
    "        if detached:\n",
    "            return get_dist(result, tanh = self.tanh), get_dist(result, detached= True, tanh = self.tanh)\n",
    "        else:\n",
    "            return get_dist(result, tanh = self.tanh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialBuilder(\n",
       "  (layers): ModuleList(\n",
       "    (0): LinearBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "        (1): Mish(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): LSTM(128, 128, num_layers=5, batch_first=True)\n",
       "    (2): LinearBlock(\n",
       "      (layers): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=20, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prior_config = dict(\n",
    "    n_blocks = 5, #self.n_processing_layers,\n",
    "    in_feature =  32, # state_dim + latent_dim \n",
    "    hidden_dim = 128, \n",
    "    out_dim = 10 * 2,\n",
    "    norm_cls = nn.LayerNorm,\n",
    "    act_cls = nn.Mish,\n",
    "    linear_cls = LinearBlock,\n",
    "    tanh = True,\n",
    "    bias = True,\n",
    "    dropout = 0,\n",
    "    module_type = \"linear\"\n",
    ")\n",
    "\n",
    "\n",
    "encoder_config = dict(\n",
    "    n_blocks = 5, #self.n_processing_layers,\n",
    "    in_feature =  32, # state_dim + latent_dim \n",
    "    hidden_dim = 128, \n",
    "    out_dim = 10 * 2,\n",
    "    norm_cls = nn.LayerNorm,\n",
    "    act_cls = nn.Mish,\n",
    "    linear_cls = LinearBlock,\n",
    "    tanh = True,\n",
    "    bias = True,\n",
    "    dropout = 0,\n",
    "    module_type = \"rnn\",\n",
    "    batch_first = True,\n",
    "    rnn_cls = nn.LSTM,\n",
    "    false = False\n",
    ")\n",
    "\n",
    "\n",
    "SequentialBuilder(encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "import hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MissingConfigException',\n",
       " 'TaskFunction',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_internal',\n",
       " 'compose',\n",
       " 'conf',\n",
       " 'core',\n",
       " 'errors',\n",
       " 'grammar',\n",
       " 'initialize',\n",
       " 'initialize_config_dir',\n",
       " 'initialize_config_module',\n",
       " 'main',\n",
       " 'plugins',\n",
       " 'types',\n",
       " 'utils',\n",
       " 'version']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hydra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skill2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
